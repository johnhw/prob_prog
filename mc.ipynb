{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic programming with PyMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets, sklearn.linear_model, sklearn.neighbors\n",
    "import sklearn.manifold, sklearn.cluster\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys, os, time\n",
    "import scipy.io.wavfile, scipy.signal\n",
    "import pymc as mc\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (18.0, 10.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic purpose\n",
    "This section will cover probabilistic **inference**. Rather than learning a single set of parameters by optimisation, we can model probability distributions over possible models that might be compatible with our data.  We'll use Monte Carlo sampling to make it simple and easy (if not very efficient) to work with probabilistic models. We will use these approaches to model **typing behaviour** at the keystroke level, and both make predictions given some data (\"how likely is it that this sequence was typed by user X?\") and quantify how much confidence we have in those models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Graphical models\n",
    "<a id=\"graphical\"> </a>\n",
    "\n",
    "Transformations of expressions to graphs is familiar to most computer scientists -- it is an essential part of most optimising compilers. For example, the equation of a straight line might be written as a graph (this is how a compiler would break down the expression):\n",
    "\n",
    "<img src=\"imgs/ymxc.png\" width=\"300px\">\n",
    "\n",
    "##### Adding unknowns\n",
    "If we have multiple dependent random variables whose distribution we want to infer, we can draw a graph of dependencies to form a *graphical model*.  This explictly models dependencies between **random variables** (i.e. ones we don't know the value of precisely) and inference can be performed on the entire graph. \n",
    "\n",
    "**In CS terms, we are writing expressions down without fixing the variables, and then allowing the distribution of the values to be inferred when we observe data.** This inference process narrows down the likely range a random variable could take on (hopefully!).\n",
    "\n",
    "In a **probabilistic graphical model**, some nodes in the graph are **observed** -- that is we know their state because we have explicity measured it, and others are **unobserved** -- we know (or have guessed) the form of their distribution but not the parameters of that distribution. Some dependencies are deterministic (i.e. fully defined by the values of their parents), while others are stochastic. We can infer the **posterior** distribution of unobserved nodes by integrating over the possible values that could have occured given the observed values.\n",
    "\n",
    "We can modify our straight line equation to write a model for **linear regression**:\n",
    "\n",
    "<img src=\"imgs/ymxc_stochastic.png\">\n",
    "\n",
    "All we need to do is specify that we expected the output $y$ to be normally distributed around the equation of a line given by $m$ and $c$; we can now **infer** $\\sigma, m, c$ from observed data. Or we can fix any of them, and infer the remainder (if, e.g. we knew in advance that $c=0$). Our assumption here is that we will observe data which has a **latent structure** modelled by a linear dependence on a variable $x$, plus some normally-distributed observation noise.\n",
    "\n",
    "**Note that we must put *some* prior distribution on every stochastic node.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Let's do it: PyMC\n",
    "<a id=\"pymc\"> </a>\n",
    "We'll use the excellent PyMC module to do the inference. If you have questions about this module, you can read [this tutorial](http://arxiv.org/abs/1507.08050) or the [API docs](https://pymc-devs.github.io/pymc/). There's a new and even nicer version with very powerful capabilities currently called **PyMC3**, but the dependencies are \"hard\" to install at the moment.\n",
    "\n",
    "Let's implement the linear regression model in the intro in practice, using PyMC to build a graphical model and then run MCMC to sample from the posterior (i.e. estimate the distribution of random variables after seeing some evidence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Bayesian Linear Regression with pymc\n",
    "### We use Monte Carlo sampling to estimate the distribution of a linear function with a normally\n",
    "### distributed error, given some observed data.\n",
    "### Vaguely based on: http://matpalm.com/blog/2012/12/27/dead_simple_pymc/ and http://sabermetricinsights.blogspot.co.uk/2014/05/bayesian-linear-regression-with-pymc.html\n",
    "\n",
    "## Utility function to plot the graph of a PyMC model\n",
    "def show_dag(model):\n",
    "    dag = mc.graph.dag(model)\n",
    "    dag.write(\"graph.png\",format=\"png\")\n",
    "    from IPython.display import Image\n",
    "    i = Image(filename='graph.png')\n",
    "    return i\n",
    "\n",
    "## generate data with a known distribution\n",
    "## this will be our \"observed\" data\n",
    "x = np.sort(np.random.uniform(0,20, (50,)))\n",
    "m = 2\n",
    "c = 15\n",
    "\n",
    "# Add on some measurement noise, with std. dev. 3.0\n",
    "epsilon = data = np.random.normal(0,3, x.shape)\n",
    "y = m * x + c + epsilon\n",
    "\n",
    "plt.plot(x,y, '.', label=\"Datapoints\")\n",
    "plt.plot(x, m*x+c, '--', lw=3, label=\"True\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.xlabel(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Now, set up the PyMC model\n",
    "## specify the prior distribution of the unknown line function variables\n",
    "## Here, we assume a normal distribution over m and c\n",
    "m_unknown = mc.Normal('m', 0, 0.01)\n",
    "c_unknown = mc.Normal('c', 0, 0.001)\n",
    "\n",
    "## specify a prior over the precision (inverse variance) of the error term\n",
    "# precision = 1/variance\n",
    "## Here we specify a uniform distribution from 0.001 to 10.0\n",
    "precision = mc.Uniform('precision', lower=0.001, upper=10.0)\n",
    "\n",
    "# specify the observed input variable\n",
    "# we use a normal distribution, but this has no effect --\n",
    "# the values are fixed and the paramters\n",
    "# never updated; this is just a way of transforming x \n",
    "# into a variable pymc can work with\n",
    "# (it's really a hack)\n",
    "x_obs = mc.Normal(\"x_obs\", 0, 1, value=x, observed=True)\n",
    "\n",
    "@mc.deterministic(plot=False)\n",
    "def line(m=m_unknown, c=c_unknown, x=x_obs):\n",
    "    return x*m+c\n",
    "\n",
    "# specify the observed output variable (note if use tau instead of sigma, we use the precision paramterisation)\n",
    "y_obs =  mc.Normal('y_obs', mu=line, tau=precision, value=y, observed=True)\n",
    "\n",
    "model = mc.Model([m_unknown, c_unknown, precision, x_obs, y_obs])\n",
    "\n",
    "# display the graphical model\n",
    "show_dag(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample from the distribution\n",
    "mcmc = mc.MCMC(model)\n",
    "mcmc.sample(iter=10000)\n",
    "\n",
    "## plot histograms of possible parameter values\n",
    "plt.figure()\n",
    "plt.hist(mcmc.trace(\"m\")[:], normed=True, bins=30)\n",
    "plt.title(\"Estimate of m\")\n",
    "plt.figure()\n",
    "plt.hist(mcmc.trace(\"c\")[:], normed=True, bins=30)\n",
    "plt.title(\"Estimate of c\")\n",
    "plt.figure()\n",
    "plt.hist(np.sqrt(1.0/mcmc.trace(\"precision\")[:]), normed=True, bins=30)\n",
    "plt.title(\"Estimate of epsilon std.dev.\")\n",
    "plt.figure()\n",
    "\n",
    "## now plot overlaid samples from the linear function\n",
    "ms = mcmc.trace(\"m\")[:]\n",
    "cs = mcmc.trace(\"c\")[:]\n",
    "\n",
    "plt.title(\"Sampled fits\")\n",
    "plt.plot(x, y, '.', label=\"Observed\")\n",
    "plt.plot(x, x*m+c, '--', label=\"True\")\n",
    "xf = np.linspace(-20,40,200)\n",
    "for m,c in zip(ms[::20], cs[::20]):    \n",
    "    plt.plot(xf, xf*m+c, 'r-', alpha=0.005)\n",
    "plt.legend()\n",
    "plt.xlim(-20,40)\n",
    "plt.ylim(-40,80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple mixture model\n",
    "We can include both **discrete** and **continuous** variables. A very important case is where we have a **mixture model**. That is, we believe our observations come from one of a number of distributions. For example, in modelling human heights, we might expect height to be normally distributed, but to have two different distributions for men and women.\n",
    "\n",
    "<img src=\"imgs/mixture.png\">\n",
    "\n",
    "It is very straightforward to add this to a PyMC graphical model; it is just another random variable to infer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Adapted from the example given at \n",
    "## http://stackoverflow.com/questions/18987697/how-to-model-a-mixture-of-3-normals-in-pymc\n",
    "\n",
    "n = 3\n",
    "ndata = 500\n",
    "\n",
    "## A Dirichlet distribution specifies the distribution over categories\n",
    "## All 1 means that every category is equally likely\n",
    "dd = mc.Dirichlet('dd', theta=(1,)*n)\n",
    "\n",
    "## This variable \"selects\" the category (i.e. the normal distribution)\n",
    "## to use. The Dirichlet distribution sets the prior over the categories.\n",
    "category = mc.Categorical('category', p=dd, size=ndata)\n",
    "\n",
    "## Now we set our priors the precision and mean of each normal distribution\n",
    "## Note the use of \"size\" to generate a **vector** of variables \n",
    "# (i.e. one for each category)\n",
    "\n",
    "## We expect the precision of each normal to be Gamma distributed \n",
    "# (this mainly forces it to be positive!)\n",
    "precs = mc.Gamma('precs', alpha=0.1, beta=0.1, size=n)\n",
    "\n",
    "## And the means of the normal to be normally distributed, with a precision of 0.001 \n",
    "# (i.e. std. dev 1000)\n",
    "means = mc.Normal('means', 0, 0.001, size=n)\n",
    "\n",
    "## These deterministic functions link the means of the observed distribution \n",
    "# to the categories\n",
    "## They just select one of the elements of the mean/precision vector, \n",
    "# given the current value of category\n",
    "## The input variables must be specified in the parameters, so that \n",
    "# PyMC knows which variables to pass to it\n",
    "@mc.deterministic\n",
    "def mean(category=category, means=means):\n",
    "    return means[category]\n",
    "\n",
    "@mc.deterministic\n",
    "def prec(category=category, precs=precs):\n",
    "    return precs[category]\n",
    "\n",
    "## Generate synthetic mixture-of-normals data, \n",
    "# with means at -50,0,+50, and std. dev of 1\n",
    "v = np.random.randint( 0, n, ndata)\n",
    "data = (v==0)*(np.random.normal(50,5,ndata)) + (v==1)*(np.random.normal(-50,5,ndata)) + (v==2)*np.random.normal(0,5,ndata)\n",
    "\n",
    "\n",
    "## Plot the original data\n",
    "plt.hist(data, bins=50)  \n",
    "\n",
    "## Now we specify the variable we observe -- which is normally distributed, *but*\n",
    "## we don't know the mean or precision. \n",
    "# Instead, we pass the **functions** mean() and pred()\n",
    "## which will be used at each sampling step.\n",
    "## We specify the observed values of this node, and tell PyMC these are observed \n",
    "## This is all that is needed to specify the model\n",
    "obs = mc.Normal('obs', mean, prec, value=data, observed = True)\n",
    "\n",
    "## Now we just bundle all the variables together for PyMC\n",
    "model = mc.Model({'dd': dd,\n",
    "              'category': category,\n",
    "              'precs': precs,\n",
    "              'means': means,\n",
    "              'obs': obs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def show_dag(model):\n",
    "    dag = mc.graph.dag(model)\n",
    "    dag.write(\"graph.png\",format=\"png\")\n",
    "    from IPython.display import Image\n",
    "    i = Image(filename='graph.png')\n",
    "    return i\n",
    "    \n",
    "show_dag(model)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mcmc = mc.MCMC(model)\n",
    "\n",
    "## Now we tell the sampler what method to use\n",
    "## Metropolis works well, but we must tell PyMC to use a specific\n",
    "## discrete sampler for the category variable to get good results in a reasonable time\n",
    "mcmc.use_step_method(mc.AdaptiveMetropolis, model.means)\n",
    "mcmc.use_step_method(mc.AdaptiveMetropolis, model.precs)\n",
    "mcmc.use_step_method(mc.DiscreteMetropolis, model.category) ## this step is key!\n",
    "mcmc.use_step_method(mc.AdaptiveMetropolis, model.dd)\n",
    "\n",
    "## Run the sampler\n",
    "mcmc.sample(iter=525000, burn=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(mcmc.trace('means').gettrace()[:], normed=True)\n",
    "plt.title(\"Estimated means\")\n",
    "plt.legend(['Component 1', 'Component 2', 'Component 3'])\n",
    "plt.figure()\n",
    "## show the result in terms of std. dev. (i.e sqrt(1.0/precision))\n",
    "plt.title(\"Estimated std. dev\")\n",
    "plt.hist(np.sqrt(1.0/mcmc.trace('precs').gettrace()[:]), normed=True)\n",
    "plt.legend(['Component 1', 'Component 2', 'Component 3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC in practice: sampling issues\n",
    "<a id=\"sampling\"> </a>\n",
    "\n",
    "The **great thing** about MCMC approaches is that you can basically write down your model and then run inference directly. There is no need to derive complex approximations, or to restrict ourselves to limited models for which we can compute answers analyitically.\n",
    "\n",
    "MCMC allows us to use distributions *we can't even sample from directly*. First we couldn't calculate the evidence P(B), so we integrated; but we couldn't solve the integral, so we sampled; but then we couldn't sample from the distribution so we used MCMC. It's a very general approach!\n",
    "\n",
    "The **bad thing** about MCMC approaches is that, even though it will do the \"right thing\" *asymptotically*, the choice of sampling strategy has a very large influence for the kind of sample runs that are practical to execute.\n",
    "\n",
    "Bayesian inference should depend only on the priors and the evidence observed; but MCMC approaches also depend on the sampling strategy used to approximate the posterior. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropolis-Hastings\n",
    "Metropolis-Hastings (or just plain Metropolis) takes a different approach, and is able to work in high-dimensional spaces. Metropolis also uses an auxiliary distribution $q(x)$, but it uses this to **wander around** in the distribution space, accepting jumps to new positions based on both $q(x)$ and $p(x)$.  This random walk (a **Markov chain**, because we make a random jump conditioned only on where we currently are) is a the \"Markov Chain\" bit of \"Markov Chain Monte Carlo\".\n",
    "\n",
    "We just take our current position $x$, and propose a new position $x^\\prime = x + x_q$, where $x_q$ is a random sample drawn from $q(x)$. This makes local steps in the space of the probability density. If $q(x)$ has a simple, symmetric form (e.g. is Gaussian), there is a very simple formula to decide whether to accept or reject a step from $p(x)$ to a new candidate position $p(x^\\prime)$:\n",
    "$$\n",
    "p(\\text{accept}) = \\begin{cases} p(x^\\prime)/p(x), & p(x)>=p(x^\\prime) \\\\  1, & p(x)<p(x^\\prime) \\end{cases} $$\n",
    "\n",
    "The asymmetric case is only slightly more involved, but it is  unusual to need to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def metropolis(p,q,x_init,n):\n",
    "    x = x_init\n",
    "    \n",
    "    samples = []\n",
    "    rejected = [] # we only keep the rejected samples to plot them later\n",
    "    for i in range(n):\n",
    "        # find a new candidate spot to jump to\n",
    "        x_prime = x + q()\n",
    "        # if it's better, go right away\n",
    "        if p(x_prime)>p(x):\n",
    "            x = x_prime\n",
    "            samples.append(x_prime)            \n",
    "        else:\n",
    "            # if not, go with probability proportional to the\n",
    "            # ratio between the new point and the current one\n",
    "            pa = p(x_prime)/p(x)\n",
    "            if np.random.uniform(0,1)<pa:\n",
    "                x = x_prime\n",
    "                samples.append(x_prime)\n",
    "            else:\n",
    "                rejected.append(x_prime)\n",
    "                \n",
    "    return np.array(samples), np.array(rejected)\n",
    "\n",
    "\n",
    "A = np.array([[0.15, 0.9], [-0.1, 2.5]])\n",
    "p = lambda x:scipy.stats.multivariate_normal(mean=[0,0], cov=A).pdf(x)\n",
    "q = lambda: np.random.normal(0,0.5,(2,))\n",
    "accept, reject = metropolis(p,q,[0.1, 0.3], 200)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(accept[:,0], accept[:,1])\n",
    "plt.plot(accept[:,0], accept[:,1], 'b.')\n",
    "plt.plot(reject[:,0], reject[:,1], 'r.')\n",
    "x,y = np.meshgrid(np.linspace(-5,5,30), np.linspace(-4,4,30))\n",
    "plt.imshow(p(np.dstack([x,y])), extent=[-4,4,-4,4], cmap='viridis')\n",
    "plt.grid(\"off\")\n",
    "plt.title(\"MCMC sampling with Metropolis-Hastings\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run the chain for longer, and plot the trace and the histogram of the variable\n",
    "accept, reject = metropolis(p,q,[0.1, 0.3], 2000)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(accept[:,0])\n",
    "plt.title(\"Trace for $x$\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(accept[:,0], bins=20)\n",
    "plt.title(\"Histogram of $x$\")\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(accept[:,1])\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Trace for $y$\")\n",
    "plt.plot(accept[:,0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Histogram of $y$\")\n",
    "plt.hist(accept[:,0], bins=20);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others\n",
    "There are many other MCMC samplers, such as:\n",
    "* **Gibbs** samplers, which are very efficient when we can sample from the conditional distribution (i.e. from one dimension of a distribution at a time), but not from the joint directly.\n",
    "* **Hamiltonian** samplers, which extend Metropolis-like steps with \"virtual physics\" which pushes the samples in sensible directions (i.e. not down the gradient of the function!)\n",
    "* **Slice** samplers, which are very clever and efficient, but only work for 1-dimensional (univariate) distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested in learning more about MCMC, David Mackay's [book chapter](http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/356.384.pdf) is a good reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "<a id=\"imputation\"> </a>\n",
    "\n",
    "In PyMC, variables can be **observed** (fixed) or **unobserved** (random). PyMC cycles through the array of known values for the **observed** variables and updates the rest of the graph.\n",
    "\n",
    "But what if you want to ask \"what if?\"-style question? For example, if you knew the last two key codes and timings, what is the distribution over the possible times for the *next* key? \n",
    "\n",
    "PyMC implements this using **imputation**, where certain missing values in an observed variable can be inferred (*imputed*) from the rest of the model. **Masked arrays** are used to implement imputation; these allow arrays to have \"blank\" values, that PyMC can fill in automatically.\n",
    "\n",
    "This approach creates one new random variable per missing data item; this can create very large models if you are not careful!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Example, using the linear regression model from the last section:\n",
    "import numpy.ma as ma # masked array support\n",
    "\n",
    "## generate the data for the regression\n",
    "x = np.sort(np.random.uniform(0,20, (50,)))\n",
    "m = 2\n",
    "c = 15\n",
    "# Add on some measurement noise, with std. dev. 3.0\n",
    "epsilon = data = np.random.normal(0,3, x.shape)\n",
    "y = m * x + c + epsilon\n",
    "\n",
    "## Now the imputation; we will try and infer missing some missing values of y (we still have the corresponding x)\n",
    "## mark last three values of y invalid\n",
    "y_impute = y[:]\n",
    "y_impute[-3:] = 0\n",
    "y_impute = ma.masked_equal(y_impute,0)\n",
    "print \"Y masked for imputation:\", y_impute # we will see the last three entries with --\n",
    "\n",
    "# create the model (exactly as before, except we switch \"y_impute\" for \"y\")\n",
    "m_unknown = mc.Normal('m', 0, 0.01)\n",
    "c_unknown = mc.Normal('c', 0, 0.001)\n",
    "precision = mc.Uniform('precision', lower=0.001, upper=10.0)\n",
    "x_obs = mc.Normal(\"x_obs\", 0, 1, value=x, observed=True)\n",
    "@mc.deterministic(plot=False)\n",
    "def line(m=m_unknown, c=c_unknown, x=x_obs):\n",
    "    return x*m+c\n",
    "y_obs =  mc.Normal('y_obs', mu=line, tau=precision, value=y_impute, observed=True)\n",
    "model = mc.Model([m_unknown, c_unknown, precision, x_obs, y_obs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample from the distribution\n",
    "mcmc = mc.MCMC(model)\n",
    "mcmc.sample(iter=10000, burn=2000, thin=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## now we will have three entries in the y_obs trace from this run\n",
    "y_trace = mcmc.trace('y_obs')[:]\n",
    "\n",
    "## the original data\n",
    "plt.plot(x[:-3],y[:-3], '.')\n",
    "plt.plot(x[-3:],y[-3:], 'go')\n",
    "plt.plot(x,x*m+c, '-')\n",
    "\n",
    "# samples from posterior predicted for the missing values of y\n",
    "plt.plot(np.tile(x[-3], (len(y_trace[:,0]), 1)), y_trace[:,0],  'r.', alpha=0.01)\n",
    "plt.plot(np.tile(x[-2], (len(y_trace[:,1]), 1)), y_trace[:,1],  'r.', alpha=0.01)\n",
    "plt.plot(np.tile(x[-1], (len(y_trace[:,2]), 1)), y_trace[:,2],  'r.', alpha=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, while it makes sense to be able to infer $x$ given $y$, as well as the $y$ given $x$ we just did, PyMC cannot automatically infer variables in this manner. It can only infer the \"forward\" path in the graph. In theory, if all the determinstic functions (like the line function) were invertible, then this reverse inference could be performed automatically without changing the model.\n",
    "\n",
    "------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
